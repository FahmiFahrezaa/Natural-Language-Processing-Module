{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b2a19c",
   "metadata": {},
   "source": [
    "# Scrape transcript film Ghostbuster Afterlife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99adb646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: text, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "page = requests.get(\"https://scrapsfromtheloft.com/movies/ghostbusters-afterlife-transcript/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "transcript = soup.find_all('div', class_='entry-content')\n",
    "\n",
    "clean_transcript = []\n",
    "for t in transcript:\n",
    "    text = t.get_text()\n",
    "    text = re.sub(r'\\[[^()]*\\]', '', text)   # menghapus tanda kurung siku\n",
    "    text = re.sub(r'\\s+', ' ', text)         # menghapus whitespace yang berlebihan\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)      # menghapus tanda baca\n",
    "    clean_transcript.append(text)\n",
    "\n",
    "corpus = clean_transcript\n",
    "df = pd.DataFrame({'text': corpus})\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())    # mengubah semua huruf menjadi lowercase\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))    # menghapus tanda baca\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))       # menghapus angka\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))      # menghapus whitespace yang berlebihan\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())   # menghapus whitespace di awal dan akhir kalimat\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))  # menghapus kata yang hanya terdiri dari 1-2 huruf\n",
    "df['text'] = df['text'].apply(lambda x: x.split())   # memecah setiap kata menjadi list\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word.isalpha()])   # menghapus kata yang mengandung angka atau simbol\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in ['the', 'and', 'to', 'of', 'in', 'that', 'it', 'you', 'for', 'this', 'with', 'is', 'on', 'was', 'at', 'not', 'have', 'are', 'be', 'as', 'they', 'we', 'he', 'she', 'or', 'but', 'an', 'my', 'me', 'so', 'if', 'out', 'do', 'from', 'about', 'all', 'were', 'what', 'up', 'which', 'when', 'there', 'then', 'who', 'your', 'just', 'can', 'now', 'like', 'one', 'into', 'been', 'go', 'had', 'him', 'no', 'back', 'know', 'see', 'did', 'get',]])\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ab7a8",
   "metadata": {},
   "source": [
    "# Scrape transcript film Wakanda Forever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "656d13b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: text, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "page = requests.get(\"https://scrapsfromtheloft.com/movies/black-panther-wakanda-forever-transcript/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "transcript = soup.find_all('div', class_='entry-content')\n",
    "\n",
    "clean_transcript = []\n",
    "for t in transcript:\n",
    "    text = t.get_text()\n",
    "    text = re.sub(r'\\[[^()]*\\]', '', text)   # menghapus tanda kurung siku\n",
    "    text = re.sub(r'\\s+', ' ', text)         # menghapus whitespace yang berlebihan\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)      # menghapus tanda baca\n",
    "    clean_transcript.append(text)\n",
    "\n",
    "corpus = clean_transcript\n",
    "df = pd.DataFrame({'text': corpus})\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())    # mengubah semua huruf menjadi lowercase\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))    # menghapus tanda baca\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))       # menghapus angka\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))      # menghapus whitespace yang berlebihan\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())   # menghapus whitespace di awal dan akhir kalimat\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))  # menghapus kata yang hanya terdiri dari 1-2 huruf\n",
    "df['text'] = df['text'].apply(lambda x: x.split())   # memecah setiap kata menjadi list\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word.isalpha()])   # menghapus kata yang mengandung angka atau simbol\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in ['the', 'and', 'to', 'of', 'in', 'that', 'it', 'you', 'for', 'this', 'with', 'is', 'on', 'was', 'at', 'not', 'have', 'are', 'be', 'as', 'they', 'we', 'he', 'she', 'or', 'but', 'an', 'my', 'me', 'so', 'if', 'out', 'do', 'from', 'about', 'all', 'were', 'what', 'up', 'which', 'when', 'there', 'then', 'who', 'your', 'just', 'can', 'now', 'like', 'one', 'into', 'been', 'go', 'had', 'him', 'no', 'back', 'know', 'see', 'did', 'get',]])\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0524bcb",
   "metadata": {},
   "source": [
    "# Scrape transcript film Ghostbuster Afterlife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab857704",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-807af3397986>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# membuat document term matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "page = requests.get(\"https://scrapsfromtheloft.com/movies/shazam-fury-of-the-gods-transcript/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "transcript = soup.find_all('div', class_='entry-content')\n",
    "\n",
    "clean_transcript = []\n",
    "for t in transcript:\n",
    "    text = t.get_text()\n",
    "    text = re.sub(r'\\[[^()]*\\]', '', text)   # menghapus tanda kurung siku\n",
    "    text = re.sub(r'\\s+', ' ', text)         # menghapus whitespace yang berlebihan\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)      # menghapus tanda baca\n",
    "    clean_transcript.append(text)\n",
    "\n",
    "    \n",
    "corpus = clean_transcript\n",
    "df = pd.DataFrame({'text': corpus})\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())    # mengubah semua huruf menjadi lowercase\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))    # menghapus tanda baca\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))       # menghapus angka\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))      # menghapus whitespace yang berlebihan\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())   # menghapus whitespace di awal dan akhir kalimat\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x))  # menghapus kata yang hanya terdiri dari 1-2 huruf\n",
    "df['text'] = df['text'].apply(lambda x: x.split())   # memecah setiap kata menjadi list\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word.isalpha()])   # menghapus kata yang mengandung angka atau simbol\n",
    "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in ['the', 'and', 'to', 'of', 'in', 'that', 'it', 'you', 'for', 'this', 'with', 'is', 'on', 'was','at', 'not', 'have', 'are', 'be', 'as', 'they', 'we', 'he', 'she', 'or', 'but', 'an', 'my', 'me', 'so', 'if', 'out', 'do', 'from', 'about', 'all', 'were', 'what', 'up', 'which', 'when', 'there', 'then', 'who', 'your', 'just', 'can', 'now', 'like', 'one', 'into', 'been', 'go', 'had', 'him', 'no', 'back', 'know', 'see', 'did', 'get',]])\n",
    "\n",
    "# membuat document term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'].apply(lambda x: ' '.join(x)))\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# menampilkan hasil document term matrix\n",
    "dtm_df = pd.DataFrame(X.toarray(), columns=terms)\n",
    "print(dtm_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc6daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
